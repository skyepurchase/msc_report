\chapter{Background}
\label{ch:background}

\emph{In this chapter the notation, phrases and concepts that are used throughout the document are explained.}
\emph{An overview of general model alignment as well as the specifics of alignment via steering adaptors is described.}
\emph{This includes a history of the different techniques and the details of the four methods used in this project.}

\emph{The history of large language models and why they are important in current research is outline.}
\emph{Additionally the challenges that are faced in interpretting these large models is explained.}
\emph{The solutions to these challenges present possible metrics that can be used to analyse the effectiveness of steering adaptors.}

\section{Notation and Concepts}

Throughout this report ``neural network (NN)'' \Sref{sec:deep-learning} and ``machine learning (ML) model'' are used interchangeably though NNs are a strict subset of ML models.
In general the term ``model'' is used to refer to the collective group and represents any program that adjusts tunable parameters to predict output from input examples.

Model ``behaviours", in general, are patterns in how the model responds to input.
This includes the desired behaviour it was trained on (such as classifying images of cats and dogs) but includes patterns in the output that were not explicitly trained for.
Desired model behaviour is considered ``positive" and undesired model behaviour is considered ``negative".
Specifically, an example of the desired behaviour is considered a ``positive example'' and an example of undesired or neutral behaviour is considered a ``negative'' example.
An example of a behaviour generally includes an input-output pairing similar to training examples however they are more specific pairings than would be using during training.

When discussing NNs the concept of a ``neuron'' relates to the abstract structure that receives a real-valued, vector input and outputs a real-value scalar based on internal, learnable weights.
In practice, this is represented by a single element of a NN layer's output vector.
Furthermore, a \emph{module} refers to a collection of one or more NN layers.
A model may be made of multiple modules or be a single module.
Modules may be ``attached'' to specific layers within a model whereby the output of the layer is passed to the module as input and the module may augment, store, or monitor the data flow in the model.
These ideas are described and discussed in more detail in the deep learning section \Sref{sec:deep-learning}.

Vectors are represented by boldface letters, $\vx, \vy, \vz$, scalars are represented by greek letters, $\alpha, \beta, \gamma$, and matrices are represented by boldface capital letters, $\mA, \mB, \mC$.
Some matrices may represent transformations or collections of feature vectors, context should disambiguate the two.
In general vectors are column vectors, $\vx = \begin{bmatrix*}
    1 & 2 & \cdots & n
\end{bmatrix*}^T$ except when a collection of vectors is represented in matrix form, in this case each row is a vector.

In a multi-layer machine learning model the output of an internal layer is an ``activation'' denoted $\va$.
A positive activation is denoted $\va^+$ and a negative activation is denoted as $\va^-$. Here, ``positive activation'' means the activation extracted from the model given a positive example as above.
The mean of a set of activations, $\mathcal{A} = \{\va_i\}_{i < n}$, is denoted $\mu_\mathcal{A} = \frac{1}{n}\sum_{i=1}^{n}\va_i$.
Frequently the set of set of activations will be the positive activation or negative activation set, in this case the mean is denoted $\mu_{\va^+}$ or $\mu_{\va^-}$ respectively.

\section{Deep Learning}
\label{sec:deep-learning}

\section{Model Alignment}

As models increase in capabilities \citep{dynabench, hle, gpt-5, grok-4} they pose an increasing risk to their users \citep{c.ai, psychosis} and potentially humanity at large \citep{survellience, deepfakes, disempowerment}.
The underlying issue is that these models may be \emph{misaligned} \citep{agent-alignment}, that is to say they do not behaviour in line with users intentions.
The problem of aligning models is referred to as the \emph{alignment problem} or, within reinforcement learning \cite{rl}, the \emph{agent alignment problem} \citep{agent-alignment}.\footnote{An \emph{agent} is a reinforcement learning term for any entity that interacts with a learning environment and updates it's internal state to better achieve a predetermined goal. In this case, the model behaves as an agent.}

\citet{agent-alignment} present the agent alignment problem and propose \emph{reward modelling} as a potential avenue to align agents.
They outline a couple of assumptions as to whether reward modelling is suitable:
\begin{itemize}[nolistsep]
    \item It is possible to sufficiently learn user intentions.
    \item It is cheaper to evaluate outcomes than produce the ``correct'' behaviour.
\end{itemize}
Working on this, \citet{rlhf} apply these ideas to large language models (LLM)s.
The goal is to transform purely predictive LLMs into assistants that are ``helpful'', ``honest'', and ``harmless''.
This is achieved by utilising human feedback on LLM output as rewards for reward modelling.
This technique paved the way for the model AI chatbot \citep{chatgpt}.
The technique of reinforcement learning from human feedback (RLHF) had been developed previously by \citet{rlhf-orig} but had not been applied to LLMs.

RLHF has been shown to be very effective in transforming the behaviour of models.
However, it is still possible for RLHF ``aligned'' models to be misaligned \citep{misgeneralization, c.ai}.
Furthermore, this approach to alignment is very costly requiring human annotators, reviewers and the costly process of finetuning.
Techniques to mitigate this have been proposed including RLxF \citep{alignment-survey}, utilising both human and AI feedback, representation finetuning \citep{reft}, and parameter efficient finetuning \citep{peft}.

Representation finetuning or more generally representation engineering \citep{steering-taxonomy} presents a promising avenue for alignment \citep{steering-clear, steering-theory, steering-taxonomy}.
Rather than requiring large amounts of human annotated data and changing model weights only the representations need editting.
This thesis focuses on \emph{steering adaptors}, a subset of representation engineering.

\section{Steering Adaptors}

The general form of a steering adaptor is a simple module that augments a layer's output.
The idea is to change the internal representation away from a harmful or misaligned concept towards one that is aligned to the users intentions.
The goal is to keep all other aspects of the representation intact so that the performance of the model is not hindered.

Rather than large amounts of annotated data or large weight matrices these techniques require a handful of positive and negative examples.
Given their lightweight nature these techniques have shown promising results \citep{steering-taxonomy, steering-theory, steering-clear, reft}.

\subsection{Contrastive Activation Addition}
\label{caa}

An intuitative approach to model intervention is to perturb the model's activations in a desired direction.
By calculating a linear direction in activation space from undesired activations towards desired ones this vector can simply be added to all activations in the model during inference.
The hope is that the model produces output that matches the desired behaviour whilst maintaining the context of the new input.

\begin{figure}
    \centering
    \captionsetup{width=.9\textwidth}
    \input{chapters/tikz/caa.tex}
\caption{Demonstration of contrastive activation addition \citep{caa}. The figure represents a simple representation space of dimension 2 with clear separability. $\mu_A = \frac{1}{\|A\|}\sum_{i \in \mathcal{I}(A)} A_i$ where $A$ is a set of activation vectors. A new point, such as the black square, is translated by the steering vector.}
    \label{fig:caa}
\end{figure}

In the simplest form consider two example inputs with desired and undesired behaviour.
Their difference gives a direction in feature space that corresponds to shifting the models output from undesired behaviour towards desired behaviour.
This is the approach proposed by \citet{activation-addition}, however, it is not robust and relies heavily on the example inputs \citep{caa}.

To improve on this approach \citet{caa} suggest using a collection of examples and calculating their mean difference in activation space.
This requires the notion of \textit{contrastive pairs}, two inputs that are similar in all ways except for the behaviour that is being changed.
Hence, this approach is known as \textit{contrastive activation addition} (CAA).
This process is demonstrated in \Fref{fig:caa}.

Formally, given a set of positive example activations $(\va_i^+)_{i\le n}$ and negative example activations $(\va_i^-)_{i\le n}$ a \textit{steering vector} for this behaviour is
\[\vv_{steer} = \frac{1}{n}\sum_{i=1}^{n}\left(\va_i^+ - \va_i^-\right).\]

Given a steering vector, $\vv_{steer}$, and a model activation during inference, $\va$, the resulting steered activation is
\begin{equation}
    \label{eq:caa}
    \va_{steered} = \va + \lambda\vv_{steer}
\end{equation}
where $\lambda$ is a user-defined parameter controlling the strength of the steering intervention.
The model activation is replaced by the steered activation during inference resulting in the model producing an output aligned with the positive examples.

This approach has a few drawbacks \citep{steerability, ace, non-linear-features} due to its assumptions.
Primarily this approach does not consider how much of a behaviour is already present.
This means the steering parameter does not fully determine the strength of the desired behaviour.
Furthermore, \citet{steerability} demonstrate that this approach is not robust across behaviours that may be steered along.
The approach assumes that concepts in activation space are linear which \citet{non-linear-features} show is not universal.
Techniques such as affine concept editting (ACE) \Sref{sec:ace} use an affine approach to overcome these drawbacks.

\subsection{Affine Concept Editting}
\label{sec:ace}

\citet{ace} claim that CAA \citep{caa} is not sufficiently general as it does not consider how much the desired behaviour is already present.
To see this consider an arbitrary activation vector $\va$ and steering direction $\vr$ encoding some behaviour.
$\va$ can be decomposed as the perpendicular and parallel components of $\vr$
\begin{equation}
    \label{eq:components}
    \begin{aligned}
        \va &= \text{proj}_\vr^{\perp}(\va) + \text{proj}_\vr^{\parallel}(\va) \\
            &= \text{proj}_\vr^{\perp}(\va) + \alpha\vr.
    \end{aligned}
\end{equation}
Adding $\lambda\vr$ as per \Eref{eq:caa} will be inconsistent as $\alpha + \lambda$ will not be equivalent across all (negative) activations.
This shows that CAA \citep{caa} does not account for how much a behaviour may already be present in an activation.

Furthemore, it is not (generally) the case that $\mathbf{0}$ represents lack of behaviour.
Instead there is some vector $\va_0$ that represents the lack of the target behaviour.
\Eref{eq:components} can incorporate this idea as follows
\begin{align*}
    \va &= \va_0 + \Delta\va \\
        &= \va_0 + \text{proj}_\vr^{\perp}(\Delta\va) + \text{proj}_\vr^{\parallel}(\Delta\va) \\
        &= \va_0 + \text{proj}_\vr^{\perp}(\Delta\va) + \alpha^\prime\vr.
\end{align*}

Removing the behaviour by setting $\alpha^\prime = 0$ yields
\begin{align*}
    \va^\prime &= \va_0 + \text{proj}_\vr^{\perp}(\Delta\va) \\
               &= \va - \text{proj}_\vr^{\parallel}(\Delta\va) \\
               &= \va - \text{proj}_\vr^{\parallel}(\va) + \text{proj}_\vr^{\parallel}(\va_0) \\
               &= \va - \text{proj}_\vr^{\parallel}(\va) + \alpha_0\vr.\footnotemark[1]
\end{align*}

\begin{figure}
    \centering
    \captionsetup{width=.9\textwidth}
    \input{chapters/tikz/ace.tex}
    \caption{A comparison of CAA \citep{caa} and affine concept editting \citep{ace}. This is a reproduction of Figure 1 in \citet{ace} with the steering towards the positive examples instead. Compared to CAA, ACE does not adjust perpendicular components but correctly adjusts those parallel to the steering direction.}
    \label{fig:ace}
\end{figure}

This represents the activation lacking the target behaviour but retaining other relevant context.
\footnotetext[1]{As $\va_0$ exists as a reference point along the steered direction.}
The behaviour can be reintroduced at any relevant strength resulting in
\begin{equation}
    \label{eq:ace}
    \va_\text{steered} = \va_0 - \text{proj}_\vr^{\parallel}(\va) + \alpha_0\vr + \alpha\vr.
\end{equation}
This process is described graphically in \Fref{fig:ace}.

Given positive example activations $(\va_i^+)_{i\le n}$ and negative example activations $(\va_i^-)_{i\le n}$ the reference point and steering direction are
\begin{align*}
    \va_0 = \frac{1}{n}\sum_{i=1}^{n}\va_i^- && \vr = \frac{1}{n}\sum_{i=1}^{n}\left(\va_i^+ - \va_i^-\right).
\end{align*}

This approach is no longer a linear edit to the activations and now includes a bias term, $\va_0$.
This is therefore affine and hence the name \textit{affine concept editting} (ACE) \citep{ace}.

\subsection{Low-rank Representation Finetuning}
\label{loreft}

Both CAA \citep{caa} and ACE \citep{ace} edit the activations in their full rank form and rely on addition (whether affine or linear).
This limits the transforms the approaches can apply to the activation space.
If the desired behaviour requires rotations or scaling of the activations these methods fail.
However, perform affine transformations to the full rank activation is costly as the dimension of the activations may be large.

\citet{reft} present a low-rank steering adaptor inspired by parameter-efficient finetuning methods such as LoRA \citep{lora}, DoRA \citep{dora} and adaptor-based methods \citep{petl}.
Unlike steering these approaches aim to finetune a model using reduced parameter counts compared to the original model.
Rather than finetuning a model this approach aims to edit the representations of the model, this is equivalent to steering.

The key insight is to steer the activations in a low-rank space.
The specific approach is based on the distributed interchange intervention\footnote{This tests whether a concept is encoded in some subspace. When working with low-rank editting this is exactly the assumption we use.} \citep{dii} with the following form
\begin{equation*}
    DII(\vx,\vy,\mR) = \vx + \mR^T(\mR\vy - \mR\vx)
\end{equation*}
where $\mR \in \mathbb{R}^{r\times d}$ is a low-rank projection matrix.

\begin{figure}
    \centering
    \captionsetup{width=.9\textwidth}
    \input{chapters/tikz/loreft.tex}
    \caption{This figure demonstrates how the low-rank representation finetuning adaptor \citep{reft} operates. Unlike methods such as LoRA \citep{lora} this does replace the layer weights but simply adds the layer output. LoReST \citep{steering-clear} behaves similarly though the adaptor has a different architecture. This is a reproduction of Figure 2(2) in \citet{reft}.}
     \label{fig:loreft}
\end{figure}

\citet{reft} suggest replacing $\mR\vy$ with an affine transformation $\mW\vx + \vb$.
Thus, the adaptor learns a transformation, $\mR\va^+ = \mW\va^- + \vb$, from negative activations to positive low-rank representations.
In this way the adaptor can learn low-rank representations of activations that encapsulate the desired behaviour and adjust the activations in a parameter efficient space.
The approach is therefore a \textit{low-rank representation finetuning} (LoReFT) adaptor.
The full adaptor is
\begin{equation}
    \label{eq:loreft}
    \va_\text{steered} = \va + \mR^T(\mW\va + \vb - \mR\va).
\end{equation}
The learnable parameters of the adaptor are $\phi = \{\mW, \mR, \vb\}$.
$\mR$ is constrained to be an orthogonal projection matrix achieved by differentiable QR decomposition.

Given a dataset of contrastive pairs $\mathcal{D} = (\va_i^-, \va_i^+)_{i\le n}$ the adaptor parameters $\phi$ are trained.
The goal is to accurately predict $\va_i^+$ given $\va_i^-$ as input.

Unlike CAA \citep{caa} and ACE \citep{ace} this approach requires paired datapoints as the adaptor needs to learn a transformation from negative examples to positive examples.
This drawback means that in the low data regime this approach is less effective than the other two approaches.
However, with sufficient data, this method is able to outperform CAA and ACE as it can utilise more complex transformations between negative and positive behaviour.
The poor performance in low data regimes is improved on by \citet{steering-clear} with their low-rank representation steering adaptor.

\subsection{Low-rank Representation Steering}
\label{lorest}

\citet{steering-clear} suggest modifying LoReFT \citep{reft} to dynamically drop low-rank dimensions and bring the learnable bias term outside of the low-rank space.
This allows the model to perform well in the low data regime by relying on linear methods similar to CAA \citep{caa} but keep the benefits of LoReFT.
By dynamically dropping dimensions the adaptor has more freedom to optimise the rank of the projection.

\citet{steering-clear} define an orthogonal projection
\begin{align*}
    \mP = \mI - \mQ\text{diag}(\vp)\mQ^T && \vp_i = \text{GumbelSoftmax}([\vl_i,0];\tau)
\end{align*}
where $\mQ \in \mathbb{R}^{r\times d}$ is a learnable low-rank projection matrix, $\vl$ is a learnable Gumbel Softmax distribution probabilities, and $\tau$ is the temperature.
As with LoReFT, $\mQ$ is an orthogonal projection achieved by differentiable QR decomposition.
In comparison to LoReFT \Eref{eq:loreft} there is no representation editting in the low-rank space.
Instead the projection acts as a method to ``zero'' the activation similar to ACE \citep{ace}.

The full adaptor is
\begin{equation}
    \va_\text{steered} = \va - (\va\mQ)\text{diag}(\vp)\mQ^T + \vb.
\end{equation}
This approach also requires paired data to train the parameters, $\phi = \{\mQ, \vl, \vb\}$.
$\mQ$ is constrained to be orthogonal through differentiable QR decomposition.

Given a dataset of contrastive pairs $\mathcal{D} = (\va_i^-, \va_i^+)_{i\le n}$ the adaptor parameters $\phi$ are trained.
The goal is to accurately predict $\va_i^+$ given $\va_i^-$ as input.

In the low data regime the adaptor can learn to drop more dimensions and rely on $\vb$ similar to CAA \citep{caa} and ACE \citep{ace}.
As more data is available the adaptor can rely more on the low-rank projection similar to LoReFT \citep{reft}.
In this way the adaptor is able to perform consistently across different data regimes.

\section{Large Language Models}

Steering and model alignment in general is not confined to large language models (LLM)s however these are currently the most widespread model in use.
LLMs aim to immitate, complete, or analyse natural language and are characterised by incredibly large numbers of parameters.
Some are as large as 1.76 trillion \citep{gpt4-count} and even small models have as many as 1 billion \citep{gemma}.

Only generative LLMs are discussed in this project.
These are models which are not trained to classify or fit a dataset in the classical sense but instead to produce more data as if it were sampled from the underlying training distribution.
In the case of LLMs this means producing coherent natural language.

The underlying technology behind modern generative LLMs is the transformer \citep{transformers} and their many derivatives \citep{linear-attention, bigbird, linformer}.

\subsection{Transformers}

Transformers \citep{transformers} are now a mainstay of modern deep learning.\footnote{This section does not aim to describe transformers in full detail but provide a sufficient background for the rest of the project.
Keywords are provided for further reading and the explanation is based on the paper by \citet{turner2023}.}
They utilise the attention mechanism to dynamically transform (sequential) input based on the surrounding context.

Attention can be considered as a learnable lookup table with queries, keys and values.
If a query and a key are similar then the corresponding value should be returned.
This can be represented as a dot-product between a matrix of queries $\mQ$ and keys $\mK$.
These are normalised to act as probabilities that a specific value is the target value.
Given a matrix of values $\mV$ attentio is represented by the following equation
\begin{equation*}
    \text{softmax}\left(\frac{\mQ\mK^T}{\sqrt{d_k}}\right)\mV.
\end{equation*}
The trick is to have $\mQ$, $\mK$ and $\mV$ all depend on the input features, this is known as ``self-attention''.
In this way $\mV$ behaves like a standard weight transformation and the softmax of $\mQ$ and $\mK$ behave like a dynamic weight transformation dependent on the input.
This allows a model to ``attend" to different parts of the input by adjusting the transformation matrices that make $\mQ, \mK$ and $\mV$.

\begin{figure}
    \centering
    \captionsetup{width=.9\textwidth}
    \input{chapters/tikz/transformer.tex}
    \caption{A diagram of the standard transformer decoder block. This is based on Figure 1 of \citet{transformers}. This is a single layer in a large language model where the output of one block is fed into the input of the next.}
    \label{fig:transformer}
\end{figure}

Modern transformers contain attention blocks each containing multiple ``attention heads'' that use the above mechanism.
This allows the model to respond dynamically to a large range of inputs.
After this attention block a standard multi-layer perceptron (MLP) is added.
This constitutes the transformer block and is visualised in \Fref{fig:transformer}.

The ability for transformers to utilise context in surrounding input values makes them particularly suited to natural language processing (NLP).
The meaning of words in a sentence depend on the words that surround it.
Furthermore, the words depend on each other in different ways depending on the context.
This is precisely how transformer attention blocks work allowing them to parse natural language far better than previous attempts.

For transformers to work on natural language the input needs to be tokenized into descrete chunks, frequently based on words.
These chunks can then be converted to unique numbers and later represented as input features.
To aid the model, the position of the token within the sentence is also encoded this is known as ``positional encoding''.
This allows the model to distinguish between the two instances of ``can'' in the sentence ``can you pass me the can''.

It is important to note that when training a model to generate natural language it must be trained without access to future tokens.
The process of hiding future tokens at a given token is called ``attention masking'' and is only applied in the attention blocks.

\section{Sparse Auto-Encoders}
\label{sec:sae}

Though the processes to build, train and use a machine learning model are known, these processes and models themselves are not fully understood.
One line of research that aims to understand how models work is ``mechanistic interpretability''.
This is the field of reverse engineering how models work, converting structures in the model into human interpretable concepts and algorithms \citep{mech-interp}.

\begin{figure}
    \centering
    \captionsetup{width=.9\textwidth}
    \input{chapters/tikz/superposition.tex}
    \caption{The three charts demonstrate the different ways a model may organise it's representation space. This figure is a reproduction of \citet{superposition} Figures 2 and 3. The privileged basis means that the representations are aligned with the architectures `preferred' basis. Polysemanticity occurs when a specific neuron is activated by two, potentially unrelated, inputs. Finally, superposition occurs when the model has to embed more representations than privileged bases resulting in forced polysemanticity.}
    \label{fig:superposition}
\end{figure}

\citet{polysemanticity} found that in vision networks certain neurons are active\footnote{output a non-zero value.} across a range of inputs.
This idea is known as ``polysemanticity'' as the neuron represents multiple semantic meanings.
This poses a problem for interpretability as it is not sufficient to assign meaning to specific neurons and check when they are active.
This phenomenon has been shown to occur in LLMs and has been demonstrated in toy examples \citep{superposition}.

\citet{superposition} propose the idea of ``superposition'' to explain why large models contain polysemantic neurons.
Superposition is the process of NNs representing more features than neurons within the model.
The features are no longer represented orthogonally in the representation space but instead share components.
This means that if only one (sparse) feature is active the non-orthogonal features will also be partially active.

The ideas of polysemanticity and superposition are represented in \Fref{fig:superposition}.

\begin{figure}
    \centering
    \captionsetup{width=.9\textwidth}
    \input{chapters/tikz/sae.tex}
    \caption{The sparse autoencoder is inserted at a specific location within the transformer block. The decoder transforms the input into a sparse vector representation and the decoder aims to reconstruct the input to feed back into the transformer. This way the sparse features do not have the superposition problem and relate to the models internal representation.}
    \label{fig:sae}
\end{figure}

To disentangle the polysemantic neurons requires eliminating the superposition present in the model.
This is a known problem known as ``sparse dictionary encoding'' \citep{sparse-coding} in neuroscience, in which a signal in superposition is decomposed into sparse elements.
\citet{sae-orig} and \citet{saes} apply the idea to NNs introducing the sparse autoencoder (SAE) which enforces sparsity in its internal representation.
The SAE module is demonstrated in \Fref{fig:sae}.

An SAE is an adaptor that takes a model layer's input and produces a replica of the layer's output.
In comparison to the model layer the SAE has a large hidden representation dimension in which sparsity is enforced.
This can be achieved in multiple ways such as clamping to the $k$ highest activations \citep{k-sparsity} or adding a sparsity regularising loss.
After training the elements of the SAE hidden dimension are given interpretations to better understand the model.
It is worth noting that SAEs have been shown to demonstrate subpar performance when used for interpretability \citep{saes-bad}, though this project uses SAEs for dataset analysis.

SAEs are challenging to train and so for the purposes of this project only pretrained SAEs are used.
\citet{saelens} provides a large collection of open source SAEs with their corresponding models.
This does limit the analysis as most models only have an SAE for a single layer.

\subsection{Metric Challenges}

As \citet{saes-bad} outline, SAEs do no provide a base truth for how the model represents concepts.
This means that using SAEs to extract concepts, or in the case of the project, using SAEs to evaluate the success of an adaptor will be imperfect.
Comparing across SAE features can give an insight into how a steering adaptor changes the models representation but not necessarily \emph{what} that new representation means as a human understandable concept.

The choice of metric is discussed in more detail in \Sref{sec:steering-clear}.
Inherent challenges with SAEs are not addressed but rather the analysis takes into account their limitations.
