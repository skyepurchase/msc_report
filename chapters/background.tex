\chapter{Background}

\section{Notation and Concepts}

Model ``behaviours", in general, are patterns in how the model responds to input.
This includes the desired behaviour it was trained on (such as classifying images of cats and dogs) but includes patterns in the output that were not explicitly trained for.
Desired model behaviour is considered ``positive" and undesired model behaviour is considered ``negative".
Specifically, an example of the desired behaviour is considered a ``positive example'' and an example of undesired or neutral behaviour is considered a ``negative'' example.
An example of a behaviour generally includes an input-output pairing similar to training examples however they are more specific pairings than would be using during training.

Vectors are represented by boldface letters, $\vx, \vy, \vz$, scalars are represented by greek letters, $\alpha, \beta, \gamma$, and matrices are represented by boldface capital letters, $\mA, \mB, \mC$.
Some matrices may represent transformations or collections of feature vectors, context should disambiguate the two.
In general vectors are column vectors, $\vx = \begin{bmatrix*}
    1 & 2 & \cdots & n
\end{bmatrix*}^T$ except when a collection of vectors is represented in matrix form, in this case each row is a vector.

In a multi-layer machine learning model the output of an internal layer is an ``activation'' denoted $\va$.
A positive activation is denoted $\va^+$ and a negative activation is denoted as $\va^-$. Here, ``positive activation'' means the activation extracted from the model given a positive example as above.

\section{Model Alignment}

\section{Model Intervention}

\subsection{Contrastive Activation Addition}
\label{caa}

An intuitative approach to model intervention is to perturb the model's activations in a desired direction.
By calculating a linear direction in activation space from undesired activations towards desired ones this vector can simply be added to all activations in the model during inference.
The hope is that the model produces output that matches the desired behaviour whilst maintaining the context of the new input.

\textcolor{red}{Add a figure demonstrating this!}

In the simplest form consider two example inputs with desired and undesired behaviour.
Their difference gives a direction in feature space that corresponds to shifting the models output from undesired behaviour towards desired behaviour.
This is the approach proposed by \citet{activation-addition}, however, it is not robust and relies heavily on the example inputs \cite{caa}.

To improve on this approach \citet{caa} suggest using a collection of examples and calculating their mean difference in activation space.
This requires the notion of \textit{contrastive pairs}, two inputs that are similar in all ways except for the behaviour that is being changed.
Hence, this approach is known as \textit{contrastive activation addition} (CAA).

Formally, given a set of positive example activations $(\va_i^+)_{i\le n}$ and negative example activations $(\va_i^-)_{i\le n}$ a \textit{steering vector} for this behaviour is
\[\vv_{steer} = \frac{1}{n}\sum_{i=1}^{n}\va_i^+ - \va_i^-.\]

Given a steering vector, $\vv_{steer}$, and a model activation during inference, $\va$, the resulting steered activation is
\begin{equation}
    \label{eq:caa}
    \va_{steered} = \va + \lambda\vv_{steer}
\end{equation}
where $\lambda$ is a user-defined parameter controlling the strength of the steering intervention.
The model activation is replaced by the steered activation during inference resulting in the model producing an output aligned with the positive examples.

This approach has a few drawbacks \cite{steerability, ace, non-linear-features} due to its assumptions.
Primarily this approach does not consider how much of a behaviour is already present.
This means the steering parameter does not fully determine the strength of the desired behaviour.
Furthermore, \citet{steerability} demonstrate that this approach is not robust across behaviours that may be steered along.
The approach assumes that concepts in activation space are linear which \citet{non-linear-features} show is not universal.
Techniques such as affine concept editting (ACE) \Sref{ace} use an affine approach to overcome these drawbacks.

\subsection{Affine Concept Editting}
\label{ace}

\citet{ace} claim that CAA \cite{caa} is not sufficiently general as it does not consider how much the desired behaviour is already present.
To see this consider an arbitrary activation vector $\va$ and steering direction $\vr$ encoding some behaviour.
$\va$ can be decomposed as the perpendicular and parallel components of $\vr$
\begin{align*}
    \va &= \text{proj}_\vr^{\perp}(\va) + \text{proj}_\vr^{\parallel}(\va) \\
        &= \text{proj}_\vr^{\perp}(\va) + \alpha\vr.
\end{align*}
Adding $\lambda\vr$ as per \Eref{eq:caa} will be inconsistent as $\alpha + \lambda$ will not be equivalent across all (negative) activations.
This shows that CAA \cite{caa} does not account for how much a behaviour may already be present in an activation.

Furthemore, it is not (generally) the case that $\mathbf{0}$ represents lack of behaviour.
Instead there is some vector $\va_0$ that represents the lack of the target behaviour.
The above equation can incorporate this idea as follows
\begin{align*}
    \va &= \va_0 + \Delta\va \\
        &= \va_0 + \text{proj}_\vr^{\perp}(\Delta\va) + \text{proj}_\vr^{\parallel}(\Delta\va) \\
        &= \va_0 + \text{proj}_\vr^{\perp}(\Delta\va) + \alpha^\prime\vr.
\end{align*}

Removing the behaviour by setting $\alpha^\prime = 0$ yields
\begin{align*}
    \va^\prime &= \va_0 + \text{proj}_\vr^{\perp}(\Delta\va) \\
               &= \va - \text{proj}_\vr^{\parallel}(\Delta\va) \\
               &= \va - \text{proj}_\vr^{\parallel}(\va) + \text{proj}_\vr^{\parallel}(\va_0) \\
               &= \va - \text{proj}_\vr^{\parallel}(\va) + \alpha_0\vr.\footnotemark[1]
\end{align*}
\footnotetext[1]{As $\va_0$ exists as a reference point along the steered direction.}
This represents the activation lacking the target behaviour but retaining other relevant context.
The behaviour can be reintroduced at any relevant strength resulting in
\begin{equation}
    \label{eq:ace}
    \va_\text{steered} = \va_0 - \text{proj}_\vr^{\parallel}(\va) + \alpha_0\vr + \alpha\vr.
\end{equation}
From \Sref{caa} the reference point, $\va_0$, is represented by the mean of the negative examples, $\frac{1}{n}\sum_{i=1}^{n}\va_i^-$.

This approach is no longer a linear edit to the activations and now includes a bias term, $\va_0$.
This is therefore affine and hence the name \textit{affine concept editting} (ACE) \cite{ace}.

\subsection{Low-rank Representation Finetuning}
\label{loreft}

Both CAA \cite{caa} and ACE \cite{ace} edit the activations in their full rank form and rely on addition (whether affine or linear).
This limits the transforms the approaches can apply to the activation space.
If the desired behaviour requires rotations or scaling of the activations these methods fail.
However, perform affine transformations to the full rank activation is costly as the dimension of the activations may be large.

\citet{reft} present a low-rank steering adaptor inspired by parameter-efficient finetuning methods such as LoRA \cite{lora}, DoRA \cite{dora} and adaptor-based methods \cite{petl}.
Unlike steering these approaches aim to finetune a model using reduced parameter counts compared to the original model.
Rather than finetuning a model this approach aims to edit the representations of the model, this is equivalent to steering.

The key insight is to steer the activations in a low-rank space.
The specific approach is based on the distributed interchange intervention\footnote{This tests whether a concept is encoded in some subspace. When working with low-rank editting this is exactly the assumption we use.} \cite{dii} with the following form
\begin{equation*}
    DII(\vx,\vy,\mR) = \vx + \mR^T(\mR\vy - \mR\vx)
\end{equation*}
where $\mR \in \mathbb{R}^{r\times d}$ is a low-rank projection matrix.

\citet{reft} suggest replacing $\mR\vy$ with an affine transformation $\mW\vx + \vb$.
Thus, the adaptor learns a transformation, $\mR\va^+ = \mW\va^- + \vb$, from negative activations to positive low-rank representations.
In this way the adaptor can learn low-rank representations of activations that encapsulate the desired behaviour and adjust the activations in a parameter efficient space.
The approach is therefore a \textit{low-rank representation finetuning} (LoReFT) adaptor.
The full adaptor is
\begin{equation}
    \label{eq:loreft}
    \va_\text{steered} = \va + \mR^T(\mW\va + \vb - \mR\va).
\end{equation}
The learnable parameters of the adaptor are $\phi = \{\mW, \mR, \vb\}$.
$\mR$ is constrained to be an orthogonal projection matrix achieved by differentiable QR decomposition.

Unlike CAA \cite{caa} and ACE \cite{ace} this approach requires paired datapoints as the adaptor needs to learn a transformation from negative examples to positive examples.
This drawback means that in the low data regime this approach is less effective than the other two approaches.
However, with sufficient data, this method is able to outperform CAA and ACE as it can utilise more complex transformations between negative and positive behaviour.
The poor performance in low data regimes is improved on by \citet{steering-clear} with their low-rank representation steering adaptor.

\subsection{Low-rank Representation Steering}
\label{lorest}

\citet{steering-clear} suggest modifying LoReFT \cite{reft} to dynamically drop low-rank dimensions and bring the learnable bias term outside of the low-rank space.
This allows the model to perform well in the low data regime by relying on linear methods similar to CAA \cite{caa} but keep the benefits of LoReFT.
By dynamically dropping dimensions the adaptor has more freedom to optimise the rank of the projection.

\citet{steering-clear} define an orthogonal projection
\begin{align*}
    \mP = \mI - \mQ\text{diag}(\vp)\mQ^T & \vp_i = \text{GumbelSoftmax}([\vl_i,0];\tau)
\end{align*}
where $\mQ \in \mathbb{R}^{r\times d}$ is a learnable low-rank projection matrix, $\vl$ is a learnable Gumbel Softmax distribution probabilities, and $\tau$ is the temperature.
As with LoReFT, $\mQ$ is an orthogonal projection achieved by differentiable QR decomposition.
In comparison to LoReFT \Eref{eq:loreft} there is no representation editting in the low-rank space.
Instead the projection acts as a method to ``zero'' the activation similar to ACE \cite{ace}.

The full adaptor is
\begin{equation}
    \va_\text{steered} = \va - (\va\mQ)\text{diag}(\vp)\mQ^T + \vb.
\end{equation}
This approach also requires paired data to train the parameters, $\phi = \{\mQ, \vl, \vb\}$.

In the low data regime the adaptor can learn to drop more dimensions and rely on $\vb$ similar to CAA \cite{caa} and ACE \cite{ace}.
As more data is available the adaptor can rely more on the low-rank projection similar to LoReFT \cite{reft}.
In this way the adaptor is able to perform consistently across different data regimes.

\section{Large Language Models}

\subsection{Transformers}

\section{Sparse Auto-Encoders}
